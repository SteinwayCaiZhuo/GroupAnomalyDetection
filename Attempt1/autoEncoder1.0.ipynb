{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* WADI_normaldata.csv: delete the first 4 lines of WADI_14days.csv  \n",
    "* sensor_map.txt: map the tedious sensor names to \"sensor0\": \"sensor126\", sensor_map.txt stores the map\n",
    "\n",
    "* use sensor0 data to do univariate autoencoder\n",
    "But the result is very bad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_normal_data():\n",
    "    data_df = pd.read_csv(\"../data/WADI/WADI_normaldata.csv\")\n",
    "    sensor_map = {}\n",
    "    with open(\"../data/WADI/sensor_map.txt\", \"r\") as f:\n",
    "        sensor_map = json.loads(f.read().replace(\"'\", '\"'))\n",
    "    data_df = data_df.rename(columns = sensor_map)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anomaly_data():\n",
    "    data_df = pd.read_csv(\"../data/WADI/WADI_attackdata_labelled.csv\")\n",
    "    sensor_map = {}\n",
    "    with open(\"../data/WADI/sensor_map.txt\", \"r\") as f:\n",
    "        sensor_map = json.loads(f.read().replace(\"'\", '\"'))\n",
    "    \n",
    "    data_df = data_df.rename(columns = sensor_map)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data = load_normal_data()\n",
    "anomaly_data = load_anomaly_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try sensor 0 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor0_normal = np.array(normal_data[\"sensor0\"])\n",
    "sensor0_anomaly = np.array(anomaly_data[\"sensor0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor0_max = max(max(sensor0_normal), max(sensor0_anomaly)).astype(\"double\")\n",
    "sensor0_min = min(min(sensor0_normal), min(sensor0_anomaly)).astype(\"double\")\n",
    "sensor0_normal_normalized = (sensor0_normal-sensor0_min)/(sensor0_max-sensor0_min)\n",
    "sensor0_anomaly_normalized = (sensor0_anomaly-sensor0_min)/(sensor0_max-sensor0_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_len = len(sensor0_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 20\n",
    "train_size = 10000\n",
    "sample_step = np.floor(normal_len/train_size).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.zeros((train_size, 1, sample_size), dtype=\"double\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_size):\n",
    "    train_x[i][0] = sensor0_normal_normalized[i*sample_step: (i*sample_step+sample_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_x[:,0,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_begin = ((1*24+11)*60+7)*60+46\n",
    "anomaly_end = ((1*24+11)*60+12)*60+15\n",
    "test_anomaly_size = 20\n",
    "sample_step_test_anomaly = np.floor((anomaly_end-anomaly_begin-sample_size)/test_anomaly_size).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_anomaly = np.zeros((test_anomaly_size, 1, sample_size), dtype=\"double\")\n",
    "for i in range(test_anomaly_size):\n",
    "    test_x_anomaly[i][0]=sensor0_anomaly_normalized[\\\n",
    "                        i*sample_step_test_anomaly:(i*sample_step_test_anomaly+sample_size)]\n",
    "test_y_anomaly = test_x_anomaly[:,0,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_normal = []\n",
    "test_x_index = np.random.choice(len(sensor0_anomaly_normalized)-sample_size, 100)\n",
    "for i in test_x_index:\n",
    "    if((i+sample_size)>=anomaly_begin and (i<=anomaly_end)):\n",
    "        continue\n",
    "    else:\n",
    "        test_x_normal.append([sensor0_anomaly_normalized[i:i+sample_size]])\n",
    "test_x_normal = np.array(test_x_normal)\n",
    "test_y_normal = test_x_normal[:,0,:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 111us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 77us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 66us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 71us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5fe2f8048>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1,20)),\n",
    "    tf.keras.layers.Dense(14, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(9, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(9, activation=tf.nn.relu),\n",
    "    \n",
    "    tf.keras.layers.Dense(9, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(14, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(20, activation=tf.nn.softmax)\n",
    "])\n",
    "model3.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.mean_squared_error,\n",
    "               metrics=[\"mse\"])\n",
    "model3.fit(train_x, train_y, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 1s 71us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 1s 72us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 1s 74us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 1s 68us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 1s 70us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 1s 75us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 1s 72us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 1s 72us/sample - loss: 0.0463 - mean_squared_error: 0.0463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b5fcdb7548>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_x, train_y, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/sample - loss: 0.0514 - mean_squared_error: 0.0514\n",
      "normal:  [0.05136388391256332, 0.05136389]\n"
     ]
    }
   ],
   "source": [
    "print(\"normal: \", model3.evaluate(test_x_normal, test_y_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 75us/sample - loss: 0.0436 - mean_squared_error: 0.0436\n",
      "anomaly:  [0.043597590178251266, 0.04359759]\n"
     ]
    }
   ],
   "source": [
    "print(\"anomaly: \", model3.evaluate(test_x_anomaly, test_y_anomaly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
