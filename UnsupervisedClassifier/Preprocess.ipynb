{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of Preprocssing\n",
    "* Remove first 4 lines of  WADI_14days.csv to WADI_normaldata.csv\n",
    "* Map original sensor name to sensor+i format, save sensor_map\n",
    "* Remove sensors: sensor 47, 48, 83, 84, 106\n",
    "* linear interp some missing data\n",
    "* Rescaling according to train_data\n",
    "* Apply PCA\n",
    "* Save to npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import interpolate\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_remove_description():\n",
    "    # for ../data/WADI/WADI_14days.csv\n",
    "    file_name = \"../../data/WADI/WADI_14days.csv\"\n",
    "    file_output_name = \"../../data/WADI/WADI_normaldata.csv\"\n",
    "    if(file_name == \"\" or file_name == None):\n",
    "        return\n",
    "    \n",
    "    data = None\n",
    "    with open(file_name, \"r\") as f:\n",
    "        data = f.readlines()\n",
    "\n",
    "    print(\"total lines: \", len(data))\n",
    "    # view the first lines of the data\n",
    "    for i in range(7):\n",
    "        print(data[i])\n",
    "    with open(file_output_name, \"w\") as f: \n",
    "        for i in range(4, len(data)):\n",
    "            f.write(data[i]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sensor_map():\n",
    "    sensor_map = {}\n",
    "    with open(\"../../data/WADI/WADI_normaldata.csv\", \"r\") as f:\n",
    "        columns = f.readline().replace(\"\\n\",\",\").split(\",\")[3:]\n",
    "        \n",
    "        for i in range(len(columns)):\n",
    "            if(i not in [47, 48, 83, 84, 106] and len(columns[i])>0):\n",
    "                sensor_map[columns[i]] = \"sensor\"+str(i)\n",
    "            else:\n",
    "                print(columns[i])\n",
    "                \n",
    "   # Add a extra sensor126 because of the difference of sensor name between 14days data and 2days data \n",
    "    sensor_map['\\\\\\\\WIN-25J4RO10SBF\\\\LOG_DATA\\\\SUTD_WADI\\\\LOG_DATA\\\\TOTAL_CONS_REQUIRED_FLOW'] = \"sensor126\"\n",
    "    with open(\"../../data/WADI/sensor_map.txt\", \"w\") as f:\n",
    "        f.write(str(sensor_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sensor_map():\n",
    "    with open(\"../../data/WADI/sensor_map.txt\", \"r\") as  f:\n",
    "        return json.loads(f.read().replace(\"'\", '\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to numpy interpolate finished\n",
      "All 0 error\n",
      "Train, test all 0 , sensor  6\n",
      "Train, test all 0 , sensor  7\n",
      "sensor  10  : After scaling to train, anomaly abs max =  2.0\n",
      "sensor  11  : After scaling to train, anomaly abs max =  2.0\n",
      "sensor  14  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  16  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  18  : After scaling to train, anomaly abs max =  2.0\n",
      "sensor:  61 : train all 0 test not 0\n",
      "sensor  68  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  69  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  71  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  72  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  74  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  84  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  87  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  91  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  92  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  93  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  94  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  95  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  96  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  110  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  112  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  113  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  114  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  115  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  116  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  117  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  118  : After scaling to train, anomaly abs max =  1.0\n",
      "sensor  120  : After scaling to train, anomaly abs max =  1.0\n",
      "scale finished\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'normal_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-79a7c3133a17>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mpc_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpc_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sum: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplained_variance_ratio_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m#print(pca.singular_values_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'normal_data' is not defined"
     ]
    }
   ],
   "source": [
    "#def to_numpy_rescale_pca():\n",
    "    # to_numpy and interpolate\n",
    "sensor_map = load_sensor_map()\n",
    "normal_df = pd.read_csv(\"../../data/WADI/WADI_normaldata.csv\")\n",
    "normal_np = normal_df[sensor_map].interpolate().to_numpy()\n",
    "anomaly_df = pd.read_csv(\"../../data/WADI/WADI_attackdata_labelled.csv\")\n",
    "anomaly_np = np.concatenate((anomaly_df[sensor_map].interpolate().to_numpy(), \\\n",
    "                             anomaly_df[\"attack\"].to_numpy().reshape(len(anomaly_df),1)),axis=1)\n",
    "print(\"to numpy interpolate finished\")\n",
    "\n",
    "# scale\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(normal_np)\n",
    "non_constant_scale_index = [i for i in range(normal_np.shape[1])\\\n",
    "               if(np.abs(scaler.data_max_[i] - scaler.data_min_[i])>0)]\n",
    "if(np.min(np.abs(scaler.data_max_ - scaler.data_min_))==0):\n",
    "    print(\"All 0 error\")\n",
    "normal_np_scaled = scaler.transform(normal_np)\n",
    "anomaly_np_scaled = np.concatenate((scaler.transform(anomaly_np[:,:-1]), \\\n",
    "                                    anomaly_np[:,-1].reshape(len(anomaly_df),1)), axis = 1)\n",
    "for i in range(normal_np.shape[1]):\n",
    "    if(i not in non_constant_scale_index):\n",
    "        if(np.max(np.abs(normal_np[:,i]))==0 \\\n",
    "           and np.max(np.abs(anomaly_np[:,i]))>0 ):\n",
    "            anomaly_np_scaled[:,i] = anomaly_np[:,i]/np.max(np.abs(anomaly_np[:,i]))\n",
    "            print(\"sensor: \", i, \": train all 0 test not 0\")\n",
    "        elif(np.max(np.abs(normal_np[:, i]))>0):\n",
    "            normal_np_scaled[:,i] = normal_np[:,i]/np.max(np.abs(normal_np[:, i]))\n",
    "            anomaly_np_scaled[:, i] = anomaly_np[:, i]/np.max(np.abs(normal_np[:, i]))\n",
    "            print(\"sensor \", i, \" : After scaling to train, anomaly abs max = \", \\\n",
    "                  np.max(np.abs(anomaly_np_scaled[:, i])))\n",
    "        else:\n",
    "            print(\"Train, test all 0 , sensor \", i)\n",
    "np.save(\"../../data/WADI/normal4.npy\", normal_np_scaled)\n",
    "np.save(\"../../data/WADI/anomaly4.npy\", anomaly_np_scaled)\n",
    "print(\"scale finished\")\n",
    "\n",
    "\n",
    "#PCA\n",
    "pc_dim=40\n",
    "pca = PCA(n_components = pc_dim)\n",
    "pca.fit(normal_np_scaled)\n",
    "print(\"sum: \", sum(pca.explained_variance_ratio_))\n",
    "#print(pca.singular_values_)\n",
    "normal_pc = pca.transform(normal_np_scaled)\n",
    "anomaly_pc = np.concatenate((pca.transform(anomaly_np_scaled[:,:-1]), \\\n",
    "                                    anomaly_np_scaled[:,-1].reshape(len(anomaly_df),1)), axis = 1)\n",
    "np.save(\"../../data/WADI/normal_pc4.npy\", normal_pc)\n",
    "np.save(\"../../data/WADI/anomaly_pc4.npy\", anomaly_pc)\n",
    "print(\"PCA finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
