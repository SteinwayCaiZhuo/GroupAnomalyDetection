{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import interpolate\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measure(test, pred, test_th = 0.02, pred_th = 0.24):\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    for i in range(len(test)):\n",
    "        if(test[i] >test_th):\n",
    "            if(pred[i]>pred_th):\n",
    "                TP+=1\n",
    "            elif(pred[i]<=pred_th):\n",
    "                FN +=1\n",
    "        elif(test[i]<=test_th):\n",
    "            if(pred[i]>pred_th):\n",
    "                FP +=1\n",
    "            elif(pred[i]<=pred_th):\n",
    "                TN+=1\n",
    "    if(TP+FP==0):\n",
    "        print(\"TP+FP==0\")\n",
    "        return (0,0,0)\n",
    "    if(TP+FN==0):\n",
    "        print(\"TP+FN==0\")\n",
    "        return (0,0,0)\n",
    "\n",
    "    pre = TP/(TP+FP)   \n",
    "    rec = TP/(TP+FN)\n",
    "    F1 = 2*pre*rec/(pre+rec)\n",
    "    #print(\"pre: \", pre, \";  rec: \", rec, \"; F1: \", F1)\n",
    "    return (pre, rec, F1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pc = np.load(\"../../data/SWaT/normal_pc.npy\")\n",
    "anomaly_pc = np.load(\"../../data/SWaT/anomaly_pc.npy\")\n",
    "if(\"cai_checkpoints\" not in os.listdir()):\n",
    "    os.mkdir(\"cai_checkpoints\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sample_step:  20 , train_size:  24745\n",
      "test_sample_step:  7 test size: 64259\n"
     ]
    }
   ],
   "source": [
    "\n",
    "normal_len = len(normal_pc)\n",
    "anomaly_len = len(anomaly_pc)\n",
    "dimension = normal_pc.shape[1]\n",
    "sample_size = 100\n",
    "\n",
    "# Train\n",
    "train_sample_step = 20\n",
    "train_size = (normal_len-sample_size)//train_sample_step\n",
    "train_index = np.arange(normal_len, train_sample_step)[:train_size]\n",
    "\n",
    "print(\"train_sample_step: \", train_sample_step, \", train_size: \", train_size)\n",
    "\n",
    "train_x = np.zeros((train_size, sample_size, dimension), dtype=\"double\")\n",
    "for i in range(train_size):\n",
    "    train_x[i, :, :] = normal_pc[i*train_sample_step: (i*train_sample_step+sample_size), :]\n",
    "\n",
    "train_y = np.zeros(train_size)\n",
    "\n",
    "test_sample_step = 7\n",
    "test_size = (anomaly_len-sample_size)//test_sample_step\n",
    "print(\"test_sample_step: \", test_sample_step, \"test size:\", test_size)\n",
    "test_index = np.array([i*test_sample_step for i in range(test_size)])\n",
    "test_x = np.zeros((test_size, sample_size, dimension), dtype = \"double\")\n",
    "for i in range(test_size):\n",
    "    test_x[i,:,:] = anomaly_pc[test_index[i]:(test_index[i]+sample_size), :-1]\n",
    "test_attack_level = np.array([np.mean(anomaly_pc[i:(i+sample_size), -1]) for i in test_index])\n",
    "test_y = test_attack_level\n",
    "\n",
    "X_train = np.concatenate((train_x, test_x), axis=0)\n",
    "y_train = np.concatenate((np.zeros(train_size), np.ones(test_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 18:11:30.938639 4635657664 deprecation.py:506] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0901 18:11:31.216025 4635657664 deprecation.py:323] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_deep_dropout_sigmoid = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(sample_size,dimension)),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)), \n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu,\\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model_deep_dropout_sigmoid.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_epoch=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_deep_dropout_sigmoid.load_weights(\"cai_checkpoints/model_deep_dropout_sigmoid_weights_epoch\"+str(prev_epoch)+\".h5\")\n",
    "\n",
    "print(\"previous:\")\n",
    "test_pred = model_deep_dropout_sigmoid.predict(test_x).ravel()\n",
    "test_pred = -np.log(1+1e-10-test_pred)\n",
    "print(\"After log:\", pd.Series(test_pred).describe())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********epoch:  1\n",
      "Epoch 1/3\n",
      "69733/69733 [==============================] - 56s 797us/sample - loss: 0.9113 - acc: 0.6450\n",
      "Epoch 2/3\n",
      "69733/69733 [==============================] - 55s 793us/sample - loss: 0.6506 - acc: 0.6451\n",
      "Epoch 3/3\n",
      "69733/69733 [==============================] - 56s 802us/sample - loss: 0.6506 - acc: 0.6451\n",
      "len test_pred:  (44986,) len test_attack_level: (44986,)\n",
      "corr in test:  [[ 1.00000000e+00 -9.84447142e-04]\n",
      " [-9.84447142e-04  1.00000000e+00]]\n",
      "after log: count    44986.000000\n",
      "mean         1.053669\n",
      "std          0.000078\n",
      "min          1.053591\n",
      "25%          1.053591\n",
      "50%          1.053591\n",
      "75%          1.053591\n",
      "max          1.053591\n",
      "dtype: float64\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  1.0535913705825806 , pre, rec, f1 = (0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "prev_epoch = 2\n",
    "for epoch in range(prev_epoch+1,prev_epoch+2):\n",
    "    print(\"***********epoch: \", epoch)\n",
    "    model_deep_dropout_sigmoid.fit(X_train, y_train, epochs=3)\n",
    "    # eval model\n",
    "    train_pred = model_deep_dropout_sigmoid.predict(train_x).ravel()\n",
    "    #train_pred_mse = np.array([mean_squared_error(train_y[i], train_pred[i]) for i in range(len(train_y))])\n",
    "    #pd.Series(train_pred_mse).describe()\n",
    "    test_pred = model_deep_dropout_sigmoid.predict(test_x).ravel()\n",
    "    #test_pred_mse = np.array([mean_squared_error(test_y[i], test_pred[i])for i in range(len(test_y))])\n",
    "    #pd.Series(test_pred_mse).describe()\n",
    "    print(\"len test_pred: \", test_pred.shape, \"len test_attack_level:\", test_attack_level.shape)\n",
    "    print(\"corr in test: \", np.corrcoef(test_pred, test_attack_level))\n",
    "    res = {}\n",
    "    check_num = 100\n",
    "    if(epoch == 0):\n",
    "        check_num = 2\n",
    "    test_pred = -np.log(1+1e-10-test_pred)\n",
    "    print(\"after log:\", pd.Series(test_pred).describe())\n",
    "    pred_mean = np.mean(test_pred)\n",
    "    pred_std = np.std(test_pred)\n",
    "    lower_bound = np.quantile(test_pred, 0.5)\n",
    "    upper_bound = np.quantile(test_pred, 0.99)\n",
    "    \n",
    "    for i in np.linspace(lower_bound,upper_bound, check_num):\n",
    "        res[str(i)] = eval_measure(test_attack_level, test_pred, test_th=1/(sample_size+1.0), pred_th=i)\n",
    "        print(\"pred_th = \",i, \", pre, rec, f1 =\",res[str(i)])\n",
    "    np.save(\"cai_checkpoints/pred_epoch\"+str(epoch)+\".npy\", test_pred)\n",
    "    with open(\"cai_checkpoints/res_epoch\"+str(epoch)+\".txt\", \"w\") as f:\n",
    "        f.write(str(res))\n",
    "    #model_deep_dropout_sigmoid.fit(X_train, y_train, epochs=2)\n",
    "    model_deep_dropout_sigmoid.save_weights(\"cai_checkpoints/model_deep_dropout_sigmoid_weights_epoch\"+str(epoch)+\".h5\")\n",
    "    #model_deep_dropout_sigmoid.fit(X_train, y_train,epochs=2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24747.000000\n",
       "mean         0.651363\n",
       "std          0.000047\n",
       "min          0.651317\n",
       "25%          0.651317\n",
       "50%          0.651317\n",
       "75%          0.651317\n",
       "max          0.651317\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(train_pred).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############## train this once, get good result\n",
    "model_small_dropout_sigmoid2 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(sample_size,dimension)),\n",
    "    #tf.keras.layers.Dense(1000, activation=tf.nn.relu, \\\n",
    "    #                      kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "     #                   activity_regularizer = tf.keras.regularizers.l1(0.01)), \n",
    "    #tf.keras.layers.Dropout(0.4),\n",
    "    ##tf.keras.layers.Dense(1000, activation=tf.nn.relu,\\\n",
    "     #                     kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "     #                   activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    #tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(40, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    #tf.keras.layers.Dropout(0.4),\n",
    "    #tf.keras.layers.Dense(10, activation=tf.nn.relu, \\\n",
    "    #                      kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "    #                    activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model_small_dropout_sigmoid2.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********epoch:  3\n",
      "Epoch 1/3\n",
      "89004/89004 [==============================] - 24s 273us/sample - loss: 0.3312 - acc: 0.9638\n",
      "Epoch 2/3\n",
      "89004/89004 [==============================] - 24s 273us/sample - loss: 0.2159 - acc: 0.9674\n",
      "Epoch 3/3\n",
      "89004/89004 [==============================] - 24s 266us/sample - loss: 0.1982 - acc: 0.9691\n",
      "len test_pred:  (64259,) len test_attack_level: (64259,)\n",
      "corr in test:  [[ 1.        -0.0311831]\n",
      " [-0.0311831  1.       ]]\n",
      "before log: count    64259.000000\n",
      "mean         0.967873\n",
      "std          0.069065\n",
      "min          0.000009\n",
      "25%          0.982601\n",
      "50%          0.984582\n",
      "75%          0.984582\n",
      "max          0.984582\n",
      "dtype: float64\n",
      "after log: count    64259.000000\n",
      "mean         3.872861\n",
      "std          0.663010\n",
      "min          0.000009\n",
      "25%          4.051316\n",
      "50%          4.172201\n",
      "75%          4.172201\n",
      "max          4.172201\n",
      "dtype: float64\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n",
      "TP+FP==0\n",
      "pred_th =  4.172200734632368 , pre, rec, f1 = (0, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "prev_epoch = 2\n",
    "for epoch in range(prev_epoch+1,prev_epoch+2):\n",
    "    print(\"***********epoch: \", epoch)\n",
    "    model_small_dropout_sigmoid2.fit(X_train, y_train, epochs=3)\n",
    "    # eval model\n",
    "    train_pred = model_small_dropout_sigmoid2.predict(train_x).ravel()\n",
    "    #train_pred_mse = np.array([mean_squared_error(train_y[i], train_pred[i]) for i in range(len(train_y))])\n",
    "    #pd.Series(train_pred_mse).describe()\n",
    "    test_pred = model_small_dropout_sigmoid2.predict(test_x).ravel()\n",
    "    #test_pred_mse = np.array([mean_squared_error(test_y[i], test_pred[i])for i in range(len(test_y))])\n",
    "    #pd.Series(test_pred_mse).describe()\n",
    "    print(\"len test_pred: \", test_pred.shape, \"len test_attack_level:\", test_attack_level.shape)\n",
    "    print(\"corr in test: \", np.corrcoef(test_pred, test_attack_level))\n",
    "    res = {}\n",
    "    check_num = 100\n",
    "    if(epoch == 0):\n",
    "        check_num = 2\n",
    "    print(\"before log:\", pd.Series(test_pred).describe())\n",
    "    test_pred_log = np.zeros(test_pred.shape)\n",
    "    for i in range(len(test_pred)):\n",
    "        test_pred_log[i] = -np.log(1-test_pred[i]+1e-10)\n",
    "        \n",
    "    print(\"after log:\", pd.Series(test_pred_log).describe())\n",
    "    pred_mean = np.mean(test_pred_log)\n",
    "    pred_std = np.std(test_pred_log)\n",
    "    lower_bound = np.quantile(test_pred_log, 0.4)\n",
    "    upper_bound = np.quantile(test_pred_log, 0.99)\n",
    "    \n",
    "    for i in np.linspace(lower_bound,upper_bound, check_num):\n",
    "        res[str(i)] = eval_measure(test_attack_level, test_pred_log, test_th=1/(sample_size+1.0), pred_th=i)\n",
    "        print(\"pred_th = \",i, \", pre, rec, f1 =\",res[str(i)])\n",
    "    #np.save(\"cai_checkpoints/pred_epoch\"+str(epoch)+\".npy\", test_pred_log)\n",
    "    #with open(\"cai_checkpoints/res_epoch\"+str(epoch)+\".txt\", \"w\") as f:\n",
    "    #    f.write(str(res))\n",
    "    #model_deep_dropout_sigmoid.fit(X_train, y_train, epochs=2)\n",
    "    #model_small_dropout_sigmoid2.save_weights(\"cai_checkpoints/model_deep_dropout_sigmoid_weights_epoch\"+str(epoch)+\".h5\")\n",
    "    #model_deep_dropout_sigmoid.fit(X_train, y_train,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a4c927e10>]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ran = np.arange(0,1,0.0001)\n",
    "quant = np.quantile(train_pred,ran)\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(ran,quant)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ran, np.quantile(test_pred, ran))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_th =  5.9425 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942503030303031 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9425060606060605 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942509090909091 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942512121212121 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942515151515152 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942518181818182 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942521212121212 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942524242424242 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942527272727273 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942530303030303 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9425333333333334 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942536363636363 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942539393939394 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942542424242424 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942545454545455 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942548484848484 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942551515151515 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942554545454545 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942557575757576 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942560606060606 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942563636363636 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942566666666667 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942569696969697 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942572727272728 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942575757575757 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942578787878788 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942581818181818 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942584848484849 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9425878787878785 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942590909090909 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942593939393939 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.94259696969697 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9426 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.94260303030303 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.94260606060606 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942609090909091 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942612121212121 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9426151515151515 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942618181818181 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942621212121212 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942624242424243 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942627272727273 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942630303030303 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942633333333333 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942636363636364 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942639393939394 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9426424242424245 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942645454545454 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942648484848485 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942651515151515 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942654545454546 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.9426575757575755 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942660606060606 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942663636363636 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942666666666667 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942669696969697 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942672727272727 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942675757575757 , pre, rec, f1 = (0.1435602613946563, 0.8747479838709677, 0.24664250692816034)\n",
      "pred_th =  5.942678787878788 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942681818181819 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9426848484848485 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942687878787879 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942690909090909 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.94269393939394 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.94269696969697 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9427 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.94270303030303 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942706060606061 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942709090909091 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9427121212121214 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942715151515151 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942718181818182 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942721212121212 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942724242424243 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942727272727272 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_th =  5.942730303030303 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942733333333333 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942736363636364 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9427393939393935 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942742424242424 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942745454545455 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942748484848485 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942751515151516 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942754545454545 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942757575757576 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942760606060606 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942763636363637 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9427666666666665 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942769696969697 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942772727272727 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942775757575758 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942778787878788 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942781818181818 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942784848484848 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942787878787879 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942790909090909 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9427939393939395 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.942796969696969 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n",
      "pred_th =  5.9428 , pre, rec, f1 = (0.9444444444444444, 0.004284274193548387, 0.00852985449071751)\n"
     ]
    }
   ],
   "source": [
    "lower_bound = 5.9425#np.quantile(test_pred_log, 0.28)\n",
    "upper_bound = 5.9428#np.quantile(test_pred_log, 0.6)\n",
    "\n",
    "for i in np.linspace(lower_bound,upper_bound, check_num):\n",
    "    res[str(i)] = eval_measure(test_attack_level, test_pred_log, test_th=1/(sample_size+1.0), pred_th=i)\n",
    "    print(\"pred_th = \",i, \", pre, rec, f1 =\",res[str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 18:11:30.938639 4635657664 deprecation.py:506] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0901 18:11:31.216025 4635657664 deprecation.py:323] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_deep_dropout_sigmoid = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(sample_size,dimension)),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)), \n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu,\\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model_deep_dropout_sigmoid.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 18:11:30.938639 4635657664 deprecation.py:506] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0901 18:11:31.216025 4635657664 deprecation.py:323] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_deep_dropout_sigmoid = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(sample_size,dimension)),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)), \n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu,\\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model_deep_dropout_sigmoid.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0901 18:11:30.938639 4635657664 deprecation.py:506] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0901 18:11:31.216025 4635657664 deprecation.py:323] From /Users/e0446225/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "model_deep_dropout_sigmoid = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(sample_size,dimension)),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)), \n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu,\\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu, \\\n",
    "                          kernel_regularizer = tf.keras.regularizers.l2(0.01),\\\n",
    "                        activity_regularizer = tf.keras.regularizers.l1(0.01)),\n",
    "    tf.keras.layers.Dropout(0.4),\n",
    "    tf.keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model_deep_dropout_sigmoid.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics=[\"accuracy\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
