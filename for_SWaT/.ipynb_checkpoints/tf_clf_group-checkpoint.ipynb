{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy import interpolate\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_measure(test, pred, test_th = 0.02, pred_th = 0.24):\n",
    "    TP, FP, TN, FN = 0,0,0,0\n",
    "    for i in range(len(test)):\n",
    "        if(test[i] >test_th):\n",
    "            if(pred[i]>pred_th):\n",
    "                TP+=1\n",
    "            elif(pred[i]<=pred_th):\n",
    "                FN +=1\n",
    "        elif(test[i]<=test_th):\n",
    "            if(pred[i]>pred_th):\n",
    "                FP +=1\n",
    "            elif(pred[i]<=pred_th):\n",
    "                TN+=1\n",
    "    if(TP+FP==0):\n",
    "        print(\"TP+FP==0\")\n",
    "        return (0,0,0)\n",
    "    if(TP+FN==0):\n",
    "        print(\"TP+FN==0\")\n",
    "        return (0,0,0)\n",
    "\n",
    "    pre = TP/(TP+FP)   \n",
    "    rec = TP/(TP+FN)\n",
    "    F1 = 2*pre*rec/(pre+rec)\n",
    "    #print(\"pre: \", pre, \";  rec: \", rec, \"; F1: \", F1)\n",
    "    return (pre, rec, F1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pc = np.load(\"../../data/SWaT/normal_pc.npy\")\n",
    "anomaly_pc = np.load(\"../../data/SWaT/anomaly_pc.npy\")\n",
    "if(\"cai_checkpoints\" not in os.listdir()):\n",
    "    os.mkdir(\"cai_checkpoints\")\n",
    "\n",
    "\n",
    "normal_len = len(normal_pc)\n",
    "anomaly_len = len(anomaly_pc)\n",
    "dimension = normal_pc.shape[1]\n",
    "sample_size = 50\n",
    "\n",
    "# Train\n",
    "train_sample_step = 2\n",
    "train_size = (normal_len-sample_size)//train_sample_step\n",
    "train_index = np.arange(normal_len, train_sample_step)[:train_size]\n",
    "\n",
    "print(\"train_sample_step: \", train_sample_step, \", train_size: \", train_size)\n",
    "\n",
    "train_x = np.zeros((train_size, sample_size, dimension), dtype=\"double\")\n",
    "for i in range(train_size):\n",
    "    train_x[i, :, :] = normal_pc[i*train_sample_step: (i*train_sample_step+sample_size), :]\n",
    "\n",
    "\n",
    "test_sample_step = 1\n",
    "test_size = (anomaly_len-sample_size)//test_sample_step\n",
    "print(\"test_sample_step: \", test_sample_step)\n",
    "test_index = np.array([i*test_sample_step for i in range(test_size)])\n",
    "test_x = np.zeros((test_size, sample_size, dimension), dtype = \"double\")\n",
    "for i in range(test_size):\n",
    "    test_x[i,:,:] = anomaly_pc[test_index[i]:(test_index[i]+sample_size), :-1]\n",
    "\n",
    "test_attack_level = np.array([np.mean(anomaly_pc[i:(i+sample_size), -1]) for i in test_index])\n",
    "X_train = np.concatenate((train_x, test_x), axis=0)\n",
    "y_train = np.concatenate((np.zeros(train_size), np.ones(test_size)))\n",
    "\n",
    "\n",
    "model_deep_dropout_sigmoid = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(sample_size,dimension)),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1000, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(40, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "])\n",
    "#adam = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "model_deep_dropout_sigmoid.compile(optimizer=\"Adam\",\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "for epoch in range(6):\n",
    "    print(\"***********epoch: \", epoch)\n",
    "    model_deep_dropout_sigmoid.fit(X_train, y_train, epochs=3)\n",
    "    # eval model\n",
    "    train_pred = model_relu_linear.predict(train_x)\n",
    "    train_pred_mse = np.array([mean_squared_error(train_y[i], train_pred[i]) for i in range(len(train_y))])\n",
    "    pd.Series(train_pred_mse).describe()\n",
    "    test_pred = model_relu_linear.predict(test_x)\n",
    "    test_pred_mse = np.array([mean_squared_error(test_y[i], test_pred[i])for i in range(len(test_y))])\n",
    "    pd.Series(test_pred_mse).describe()\n",
    "    print(\"corr in test: \", np.corrcoef(test_pred_mse, test_attack_level))\n",
    "    res = {}\n",
    "    for i in np.linspace(np.mean(test_pred_mse)-np.std(test_pred_mse),np.mean(test_pred_mse)+3*np.std(test_pred_mse), 100):\n",
    "        res[str(i)] = eval_measure(test_attack_level, test_pred_mse, test_th=1/(sample_size+1.0), pred_th=i)\n",
    "        print(\"pred_th = \",i, \", pre, rec, f1 =\",res[str(i)])\n",
    "    np.save(\"cai_checkpoints/pred_epoch\"+str(epoch)+\".npy\", y_pred)\n",
    "    with open(\"cai_checkpoints/res_epoch\"+str(epoch)+\".txt\", \"w\") as f:\n",
    "        f.write(str(res))\n",
    "    model_deep_dropout_sigmoid.save_weights(\"cai_checkpoints/model_deep_dropout_sigmoid_weights_epoch\"+str(epoch)+\".h5\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
